{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to build a deep learning model to predict Gender from name\n",
    "\n",
    "We will follow the following steps in this notebook.\n",
    "1. Download the data set\n",
    "2. Explore and pre-process the dataset\n",
    "3. Showcase the encoding  (names, character-integer encoding, character-one-hot encoding)\n",
    "4. Submit Sagemaker training job\n",
    "\n",
    "\n",
    "#### Step 1: Download the data from https://www.ssa.gov/oact/babynames/names.zip\n",
    "When you unzip the download, you will find several files with names 'yob1880.txt'. \n",
    "The naming convention of this file is 'yob' stands for 'Year of Birth' and the year. \n",
    "Which means, each file contains the popular names of babies born in that year.\n",
    "\n",
    "We will first create a folder called data. Download and unzip the file. We will then proceed to \n",
    "extract the content of all those files into a single file named 'allnames.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-03-21 16:26:10--  https://www.ssa.gov/oact/babynames/names.zip\n",
      "Resolving www.ssa.gov (www.ssa.gov)... 137.200.4.16, 2001:1930:d07::aaaa\n",
      "Connecting to www.ssa.gov (www.ssa.gov)|137.200.4.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8207194 (7.8M) [application/zip]\n",
      "Saving to: ‘names.zip’\n",
      "\n",
      "names.zip           100%[===================>]   7.83M  5.09MB/s    in 1.5s    \n",
      "\n",
      "2018-03-21 16:26:12 (5.09 MB/s) - ‘names.zip’ saved [8207194/8207194]\n",
      "\n",
      "Archive:  names.zip\n",
      "  inflating: yob1884.txt             \n",
      "  inflating: yob1885.txt             \n",
      "  inflating: yob1886.txt             \n",
      "  inflating: yob1887.txt             \n",
      "  inflating: yob1888.txt             \n",
      "  inflating: yob1889.txt             \n",
      "  inflating: yob1890.txt             \n",
      "  inflating: yob1891.txt             \n",
      "  inflating: yob1892.txt             \n",
      "  inflating: yob1893.txt             \n",
      "  inflating: yob1894.txt             \n",
      "  inflating: yob1895.txt             \n",
      "  inflating: yob1896.txt             \n",
      "  inflating: yob1897.txt             \n",
      "  inflating: yob1898.txt             \n",
      "  inflating: yob1899.txt             \n",
      "  inflating: yob1900.txt             \n",
      "  inflating: yob1901.txt             \n",
      "  inflating: yob1902.txt             \n",
      "  inflating: yob1903.txt             \n",
      "  inflating: yob1904.txt             \n",
      "  inflating: yob1905.txt             \n",
      "  inflating: yob1906.txt             \n",
      "  inflating: yob1907.txt             \n",
      "  inflating: yob1908.txt             \n",
      "  inflating: yob1909.txt             \n",
      "  inflating: yob1910.txt             \n",
      "  inflating: yob1911.txt             \n",
      "  inflating: yob1912.txt             \n",
      "  inflating: yob1913.txt             \n",
      "  inflating: yob1914.txt             \n",
      "  inflating: yob1915.txt             \n",
      "  inflating: yob1916.txt             \n",
      "  inflating: yob1917.txt             \n",
      "  inflating: yob1918.txt             \n",
      "  inflating: yob1919.txt             \n",
      "  inflating: yob1920.txt             \n",
      "  inflating: yob1921.txt             \n",
      "  inflating: yob1922.txt             \n",
      "  inflating: yob1923.txt             \n",
      "  inflating: yob1924.txt             \n",
      "  inflating: yob1925.txt             \n",
      "  inflating: yob1926.txt             \n",
      "  inflating: yob1927.txt             \n",
      "  inflating: yob1928.txt             \n",
      "  inflating: yob1929.txt             \n",
      "  inflating: yob1930.txt             \n",
      "  inflating: yob1931.txt             \n",
      "  inflating: yob1932.txt             \n",
      "  inflating: yob1933.txt             \n",
      "  inflating: yob1934.txt             \n",
      "  inflating: yob1935.txt             \n",
      "  inflating: yob1936.txt             \n",
      "  inflating: yob1937.txt             \n",
      "  inflating: yob1938.txt             \n",
      "  inflating: yob1939.txt             \n",
      "  inflating: yob1940.txt             \n",
      "  inflating: yob1941.txt             \n",
      "  inflating: yob1942.txt             \n",
      "  inflating: yob1943.txt             \n",
      "  inflating: yob1944.txt             \n",
      "  inflating: yob1945.txt             \n",
      "  inflating: yob1946.txt             \n",
      "  inflating: yob1947.txt             \n",
      "  inflating: yob1948.txt             \n",
      "  inflating: yob1949.txt             \n",
      "  inflating: yob1950.txt             \n",
      "  inflating: yob1951.txt             \n",
      "  inflating: yob1952.txt             \n",
      "  inflating: yob1953.txt             \n",
      "  inflating: yob1954.txt             \n",
      "  inflating: yob1955.txt             \n",
      "  inflating: yob1956.txt             \n",
      "  inflating: yob1957.txt             \n",
      "  inflating: yob1958.txt             \n",
      "  inflating: yob1959.txt             \n",
      "  inflating: yob1960.txt             \n",
      "  inflating: yob1961.txt             \n",
      "  inflating: yob1962.txt             \n",
      "  inflating: yob1963.txt             \n",
      "  inflating: yob1964.txt             \n",
      "  inflating: yob1965.txt             \n",
      "  inflating: yob1966.txt             \n",
      "  inflating: yob1967.txt             \n",
      "  inflating: yob1968.txt             \n",
      "  inflating: yob1969.txt             \n",
      "  inflating: yob1970.txt             \n",
      "  inflating: yob1971.txt             \n",
      "  inflating: yob1972.txt             \n",
      "  inflating: yob1973.txt             \n",
      "  inflating: yob1974.txt             \n",
      "  inflating: yob1975.txt             \n",
      "  inflating: yob1976.txt             \n",
      "  inflating: yob1977.txt             \n",
      "  inflating: yob1978.txt             \n",
      "  inflating: yob1979.txt             \n",
      "  inflating: yob1980.txt             \n",
      "  inflating: yob1981.txt             \n",
      "  inflating: yob1982.txt             \n",
      "  inflating: yob1983.txt             \n",
      "  inflating: yob1984.txt             \n",
      "  inflating: yob1985.txt             \n",
      "  inflating: yob1986.txt             \n",
      "  inflating: yob1987.txt             \n",
      "  inflating: yob1988.txt             \n",
      "  inflating: yob1989.txt             \n",
      "  inflating: yob1990.txt             \n",
      "  inflating: yob1991.txt             \n",
      "  inflating: yob1992.txt             \n",
      "  inflating: yob1993.txt             \n",
      "  inflating: yob1994.txt             \n",
      "  inflating: yob1995.txt             \n",
      "  inflating: yob1996.txt             \n",
      "  inflating: yob1997.txt             \n",
      "  inflating: yob1998.txt             \n",
      "  inflating: yob1999.txt             \n",
      "  inflating: yob2000.txt             \n",
      "  inflating: yob2001.txt             \n",
      "  inflating: yob2002.txt             \n",
      "  inflating: yob2003.txt             \n",
      "  inflating: yob2004.txt             \n",
      "  inflating: yob2005.txt             \n",
      "  inflating: yob2006.txt             \n",
      "  inflating: yob2007.txt             \n",
      "  inflating: yob2008.txt             \n",
      "  inflating: yob2009.txt             \n",
      "  inflating: yob2010.txt             \n",
      "  inflating: yob2011.txt             \n",
      "  inflating: yob2012.txt             \n",
      "  inflating: yob2013.txt             \n",
      "  inflating: yob2014.txt             \n",
      "  inflating: yob2015.txt             \n",
      "  inflating: yob2016.txt             \n",
      "  inflating: yob1880.txt             \n",
      "  inflating: yob1881.txt             \n",
      "  inflating: yob1882.txt             \n",
      "  inflating: yob1883.txt             \n",
      "  inflating: NationalReadMe.pdf      \n"
     ]
    }
   ],
   "source": [
    "! mkdir data ;  cd data ; wget https://www.ssa.gov/oact/babynames/names.zip ; unzip names.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mv data/yob2016.txt data/test_data.txt\n",
    "! cat data/yob* > data/allnames.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "\n",
    "filename = 'data/allnames.txt'\n",
    "df=pd.read_csv(filename, sep=',', names = [\"Name\", \"Gender\", \"Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the data size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1859026, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 189K rows and 3 columns. Now lets see how the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna</td>\n",
       "      <td>F</td>\n",
       "      <td>2604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emma</td>\n",
       "      <td>F</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>F</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Minnie</td>\n",
       "      <td>F</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Margaret</td>\n",
       "      <td>F</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ida</td>\n",
       "      <td>F</td>\n",
       "      <td>1472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alice</td>\n",
       "      <td>F</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bertha</td>\n",
       "      <td>F</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>F</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name Gender  Count\n",
       "0       Mary      F   7065\n",
       "1       Anna      F   2604\n",
       "2       Emma      F   2003\n",
       "3  Elizabeth      F   1939\n",
       "4     Minnie      F   1746\n",
       "5   Margaret      F   1578\n",
       "6        Ida      F   1472\n",
       "7      Alice      F   1414\n",
       "8     Bertha      F   1320\n",
       "9      Sarah      F   1288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set has 3 columns, Name, Gender, and count. Here Count is the number of times this name was registered with the \n",
    "United States social security department. The names sound familiar for United states. Since we collected data\n",
    "from all 50 states, there might be some names that occur multiple times. Lets us check how many time Mary occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>6919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3935</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>8148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>8012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7407</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8146</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name Gender  Count\n",
       "0     Mary      F   7065\n",
       "1273  Mary      M     27\n",
       "2000  Mary      F   6919\n",
       "3238  Mary      M     29\n",
       "3935  Mary      F   8148\n",
       "5276  Mary      M     30\n",
       "6062  Mary      F   8012\n",
       "7407  Mary      M     32\n",
       "8146  Mary      F   9217\n",
       "9610  Mary      M     36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Name'] == 'Mary'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at sample data\n",
    "The name 'Mary' occurs multple times, and at the same time Mary is also \n",
    "listed as a Male. In the early 20th century Mary used to be a\n",
    "common name for boys, and it somewhat related to Mario.\n",
    "But, looking at the counts, Mary is much more popular \n",
    "as a female name than a male name. So, it is not possible to \n",
    "guess the gender of a person by just looking at it. \n",
    "\n",
    "The second problem is that, the name Mary appears multple times \n",
    "in the dataset. We will remove redundant entries. \n",
    "But before we remove redundant entries, we will drop the counts as \n",
    "we will not be using it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since we do not need the 'count' lets drop it from the dataframe\n",
    "df = df.drop(['Count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#checking the presence of Mary again\n",
    "df.loc[df['Name'] == 'Mary']\n",
    "\n",
    "#lets shuffle the data set\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names in the training dataset 105431\n"
     ]
    }
   ],
   "source": [
    "# lets find the number of rows we have now. We want to \n",
    "# have a reasonable number to rows to train our deep learning model\n",
    "num_names = df.shape[0]\n",
    "print ('Number of names in the training dataset', num_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest name: 15\n"
     ]
    }
   ],
   "source": [
    "# Find the longest name\n",
    "max_name_length = (df['Name'].map(len).max())\n",
    "print(\"Longest name:\", max_name_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir namesdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('namesdata/train_names.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = 'data/test_data.txt'\n",
    "df_test=pd.read_csv(test_file, sep=',', names = [\"Name\", \"Gender\", \"Count\"])\n",
    "df_test = df_test.drop(['Count'], axis=1)\n",
    "df_test.to_csv('namesdata/test_names.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32868, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption\n",
    "Beyond this point, this model will assume that the names only contain\n",
    "english alphabets (26). The algorithm has to be modified slightly if you \n",
    "use the same model for other languages.\n",
    "\n",
    "# One hot encoding of characters\n",
    "We cannot use the character symbols as is to send as input to the neural network,\n",
    "so we will convert this into a one-hot encoded sequence, based on the mapping.\n",
    "\n",
    "First lets encode the character as integer and then encode the integers into one-hot \n",
    "In one-hot encodeing a is represented as an array with the first column selected and so on \n",
    "\n",
    "a => [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "\n",
    "e => [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets define a dictionar to help us with char to integer encoding\n",
    "char_to_int = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X will be the input to the neural network, is a 3D numpyarray.\n",
    "# X is initialized with zeros\n",
    "alphabet_size = 26\n",
    "names = df['Name'].values\n",
    "genders = df['Gender']\n",
    "X = np.zeros((num_names, max_name_length, alphabet_size))\n",
    "\n",
    "# we will in each column we will encode 1 in in the column that represents the character\n",
    "for i,name in enumerate(names):\n",
    "    name = name.lower()\n",
    "    for t, char in enumerate(name):\n",
    "        X[i, t,char_to_int[char]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first name is:  Kaben\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the first name\n",
    "# every name will be of the same size 26 x 15. IN case of the \n",
    "# first name 'Mary' only the first 4 letters will be encoded\n",
    "# the rest of the rows will be all zeros\n",
    "\n",
    "print ('first name is: ', names[0])\n",
    "X[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets encode the gender in a numpy array Y\n",
    "Y = np.ones((num_names,1))\n",
    "Y[df['Gender'] == 'F',0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training job setup\n",
    "The above exercise was only to show you how to create the input and target \n",
    "for the model. We will not be training in this notebook instance, but will \n",
    "submit a training job to sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-514539217087\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='namesdata', key_prefix='namesdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# todo draw a picture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-514539217087\n",
      "INFO:sagemaker:Creating training-job with name: tf-names-2018-03-21-17-49-32-325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:TensorBoard 0.1.7 at http://localhost:6006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................................\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:33,589 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:33,589 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:36,577 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:37,891 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:38,013 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:38,081 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:38,081 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:38,082 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-1-514539217087/tf-names-2018-03-21-17-49-32-325/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe868b5a290>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_num_worker_replicas': 1, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:38,082 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:38,082 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mMonitors are deprecated. Please use tf.train.SessionRunHook.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /opt/ml/code/gender-keras-2.py:47: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPlease switch to tf.train.get_global_step\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:41.525331: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:41.664783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:41.665148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \u001b[0m\n",
      "\u001b[31mname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8755\u001b[0m\n",
      "\u001b[31mpciBusID: 0000:00:1e.0\u001b[0m\n",
      "\u001b[31mtotalMemory: 11.17GiB freeMemory: 11.10GiB\u001b[0m\n",
      "\u001b[31m2018-03-21 17:57:41.665177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-us-east-1-514539217087/tf-names-2018-03-21-17-49-32-325/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.69272447, step = 1\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.7548\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.66641647, step = 101 (6.778 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4193\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6084709, step = 201 (6.485 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4119\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6835346, step = 301 (6.489 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.423\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6078791, step = 401 (6.484 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.442\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.67638993, step = 501 (6.476 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4251\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6373418, step = 601 (6.483 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3712\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.7050004, step = 701 (6.506 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4229\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6982756, step = 801 (6.484 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4556\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6125675, step = 901 (6.470 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Starting evaluation at 2018-03-21-17:58:58\u001b[0m\n",
      "\u001b[31m2018-03-21 17:58:58.883399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-1-514539217087/tf-names-2018-03-21-17-49-32-325/checkpoints/model.ckpt-1\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [1/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [2/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [3/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [4/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [5/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [6/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [7/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [8/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [9/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [10/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Finished evaluation at 2018-03-21-17:59:01\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving dict for global step 1: accuracy = 0.0, global_step = 1, loss = 0.69888407\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Validation (step 1000): loss = 0.69888407, global_step = 1, accuracy = 0.0\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 7.34398\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6682925, step = 1001 (13.617 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4231\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.63676125, step = 1101 (6.484 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4316\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6686876, step = 1201 (6.480 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4088\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.62530684, step = 1301 (6.490 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3689\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6745179, step = 1401 (6.506 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4305\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6812636, step = 1501 (6.481 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4194\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.620754, step = 1601 (6.835 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.614\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.65807146, step = 1701 (6.493 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4105\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6825104, step = 1801 (6.489 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4061\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.66567695, step = 1901 (6.491 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.522\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6250324, step = 2001 (6.886 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3928\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.63779294, step = 2101 (6.496 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4005\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.63292056, step = 2201 (6.494 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3928\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.701165, step = 2301 (6.496 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4173\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.71074414, step = 2401 (6.486 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.378\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6274005, step = 2501 (6.503 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3907\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6131525, step = 2601 (6.497 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3946\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6140127, step = 2701 (6.496 sec)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.9729\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.69034934, step = 2801 (6.679 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.2957\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.67213434, step = 2901 (6.538 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.3294\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.66357255, step = 3001 (6.979 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3856\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.65175617, step = 3101 (6.500 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4313\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.59832513, step = 3201 (6.480 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3885\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.5295191, step = 3301 (6.498 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3998\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.555522, step = 3401 (6.493 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4156\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.58799, step = 3501 (6.875 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.545\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6346953, step = 3601 (6.488 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4099\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.5905069, step = 3701 (6.489 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4125\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.5342973, step = 3801 (6.488 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4107\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.5214073, step = 3901 (6.489 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 4000 into s3://sagemaker-us-east-1-514539217087/tf-names-2018-03-21-17-49-32-325/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Loss for final step: 0.533633.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Starting evaluation at 2018-03-21-18:02:25\u001b[0m\n",
      "\u001b[31m2018-03-21 18:02:25.902666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-1-514539217087/tf-names-2018-03-21-17-49-32-325/checkpoints/model.ckpt-4000\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [1/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [2/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [3/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [4/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [5/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [6/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [7/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [8/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [9/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [10/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Finished evaluation at 2018-03-21-18:02:28\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving dict for global step 4000: accuracy = 0.0, global_step = 4000, loss = 0.38143545\u001b[0m\n",
      "\u001b[31m2018-03-21 18:02:29.505081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-1-514539217087/tf-names-2018-03-21-17-49-32-325/checkpoints/model.ckpt-4000\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:SavedModel written to: s3://sagemaker-us-east-1-514539217087/tf-names-2018-03-21-17-49-32-325/checkpoints/export/Servo/temp-1521655349/saved_model.pb\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:writing success training\u001b[0m\n",
      "\u001b[31m2018-03-21 18:02:34,990 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-21 18:02:35,168 INFO - tf_container.serve - Downloaded saved model at /opt/ml/model/export/Servo/1521655349/saved_model.pb\u001b[0m\n",
      "\u001b[31m2018-03-21 18:02:35,184 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): s3.amazonaws.com\u001b[0m\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "gender_estimator = TensorFlow(entry_point='highlevel-tensorflow-helper.py',\n",
    "                               role=role,\n",
    "                               training_steps= 4000,                                  \n",
    "                               evaluation_steps= 10,\n",
    "                               hyperparameters={'learning_rate': 0.01},\n",
    "                               train_instance_count=1,\n",
    "                               train_instance_type='ml.p2.xlarge',\n",
    "                               base_job_name='tf-names')\n",
    "\n",
    "gender_estimator.fit(inputs, run_tensorboard_locally=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To load a preexisting model\n",
    "\n",
    "#from sagemaker.tensorflow import TensorFlowPredictor\n",
    "#predictor = TensorFlowPredictor('sagemaker-tensorflow-py2-cpu-2018-03-09-19-27-43-438')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: tensorboard-names-2018-03-20-22-40-47-154\n",
      "INFO:sagemaker:Creating endpoint with name tensorboard-names-2018-03-20-22-40-47-154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "gender_predictor = gender_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: tensorboard-names-2018-03-20-19-18-18-841\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(gender_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name1': 'pratap', 'name2': 'swetha'}\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "data['name'] = 'pratap'\n",
    "json_obj = json.loads('{\"names\": {\"name1\":\"pratap\",\"name2\":\"swetha\"}}')\n",
    "json_data = json.dumps(data)\n",
    "print (json_obj['names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ContentType\": \"*/*\",\n",
      "    \"InvokedProductionVariant\": \"AllTraffic\"\n",
      "}\n",
      "{\n",
      "  \"outputs\": {\n",
      "    \"Gender\": {\n",
      "      \"dtype\": \"DT_FLOAT\", \n",
      "      \"floatVal\": [\n",
      "        0.8597514033317566\n",
      "      ], \n",
      "      \"tensorShape\": {\n",
      "        \"dim\": [\n",
      "          {\n",
      "            \"size\": \"1\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!rm output.json\n",
    "!aws sagemaker-runtime invoke-endpoint --endpoint-name tensorboard-names-2018-03-20-22-40-47-154 --body '{\"name\":\"swetha\"}' --content-type \"application/json\" output.json\n",
    "! cat output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowPredictor\n",
    "predictor = TensorFlowPredictor('tensorflowgendermodel571')\n",
    "sagemaker.Session().delete_endpoint(predictor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
