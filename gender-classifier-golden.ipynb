{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to build a deep learning model to predict Gender from name\n",
    "\n",
    "We will follow the following steps in this notebook.\n",
    "1. Download the data set\n",
    "2. Explore and pre-process the dataset\n",
    "3. Showcase the encoding  (names, character-integer encoding, character-one-hot encoding)\n",
    "4. Submit Sagemaker training job\n",
    "\n",
    "\n",
    "#### Step 1: Download the data from https://www.ssa.gov/oact/babynames/names.zip\n",
    "When you unzip the download, you will find several files with names 'yob1880.txt'. \n",
    "The naming convention of this file is 'yob' stands for 'Year of Birth' and the year. \n",
    "Which means, each file contains the popular names of babies born in that year.\n",
    "\n",
    "We will first create a folder called data. Download and unzip the file. We will then proceed to \n",
    "extract the content of all those files into a single file named 'allnames.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-03-20 19:04:05--  https://www.ssa.gov/oact/babynames/names.zip\n",
      "Resolving www.ssa.gov (www.ssa.gov)... 137.200.4.16, 2001:1930:e03::aaaa\n",
      "Connecting to www.ssa.gov (www.ssa.gov)|137.200.4.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8207194 (7.8M) [application/zip]\n",
      "Saving to: ‘names.zip’\n",
      "\n",
      "names.zip           100%[===================>]   7.83M  5.09MB/s    in 1.5s    \n",
      "\n",
      "2018-03-20 19:04:07 (5.09 MB/s) - ‘names.zip’ saved [8207194/8207194]\n",
      "\n",
      "Archive:  names.zip\n",
      "  inflating: yob1884.txt             \n",
      "  inflating: yob1885.txt             \n",
      "  inflating: yob1886.txt             \n",
      "  inflating: yob1887.txt             \n",
      "  inflating: yob1888.txt             \n",
      "  inflating: yob1889.txt             \n",
      "  inflating: yob1890.txt             \n",
      "  inflating: yob1891.txt             \n",
      "  inflating: yob1892.txt             \n",
      "  inflating: yob1893.txt             \n",
      "  inflating: yob1894.txt             \n",
      "  inflating: yob1895.txt             \n",
      "  inflating: yob1896.txt             \n",
      "  inflating: yob1897.txt             \n",
      "  inflating: yob1898.txt             \n",
      "  inflating: yob1899.txt             \n",
      "  inflating: yob1900.txt             \n",
      "  inflating: yob1901.txt             \n",
      "  inflating: yob1902.txt             \n",
      "  inflating: yob1903.txt             \n",
      "  inflating: yob1904.txt             \n",
      "  inflating: yob1905.txt             \n",
      "  inflating: yob1906.txt             \n",
      "  inflating: yob1907.txt             \n",
      "  inflating: yob1908.txt             \n",
      "  inflating: yob1909.txt             \n",
      "  inflating: yob1910.txt             \n",
      "  inflating: yob1911.txt             \n",
      "  inflating: yob1912.txt             \n",
      "  inflating: yob1913.txt             \n",
      "  inflating: yob1914.txt             \n",
      "  inflating: yob1915.txt             \n",
      "  inflating: yob1916.txt             \n",
      "  inflating: yob1917.txt             \n",
      "  inflating: yob1918.txt             \n",
      "  inflating: yob1919.txt             \n",
      "  inflating: yob1920.txt             \n",
      "  inflating: yob1921.txt             \n",
      "  inflating: yob1922.txt             \n",
      "  inflating: yob1923.txt             \n",
      "  inflating: yob1924.txt             \n",
      "  inflating: yob1925.txt             \n",
      "  inflating: yob1926.txt             \n",
      "  inflating: yob1927.txt             \n",
      "  inflating: yob1928.txt             \n",
      "  inflating: yob1929.txt             \n",
      "  inflating: yob1930.txt             \n",
      "  inflating: yob1931.txt             \n",
      "  inflating: yob1932.txt             \n",
      "  inflating: yob1933.txt             \n",
      "  inflating: yob1934.txt             \n",
      "  inflating: yob1935.txt             \n",
      "  inflating: yob1936.txt             \n",
      "  inflating: yob1937.txt             \n",
      "  inflating: yob1938.txt             \n",
      "  inflating: yob1939.txt             \n",
      "  inflating: yob1940.txt             \n",
      "  inflating: yob1941.txt             \n",
      "  inflating: yob1942.txt             \n",
      "  inflating: yob1943.txt             \n",
      "  inflating: yob1944.txt             \n",
      "  inflating: yob1945.txt             \n",
      "  inflating: yob1946.txt             \n",
      "  inflating: yob1947.txt             \n",
      "  inflating: yob1948.txt             \n",
      "  inflating: yob1949.txt             \n",
      "  inflating: yob1950.txt             \n",
      "  inflating: yob1951.txt             \n",
      "  inflating: yob1952.txt             \n",
      "  inflating: yob1953.txt             \n",
      "  inflating: yob1954.txt             \n",
      "  inflating: yob1955.txt             \n",
      "  inflating: yob1956.txt             \n",
      "  inflating: yob1957.txt             \n",
      "  inflating: yob1958.txt             \n",
      "  inflating: yob1959.txt             \n",
      "  inflating: yob1960.txt             \n",
      "  inflating: yob1961.txt             \n",
      "  inflating: yob1962.txt             \n",
      "  inflating: yob1963.txt             \n",
      "  inflating: yob1964.txt             \n",
      "  inflating: yob1965.txt             \n",
      "  inflating: yob1966.txt             \n",
      "  inflating: yob1967.txt             \n",
      "  inflating: yob1968.txt             \n",
      "  inflating: yob1969.txt             \n",
      "  inflating: yob1970.txt             \n",
      "  inflating: yob1971.txt             \n",
      "  inflating: yob1972.txt             \n",
      "  inflating: yob1973.txt             \n",
      "  inflating: yob1974.txt             \n",
      "  inflating: yob1975.txt             \n",
      "  inflating: yob1976.txt             \n",
      "  inflating: yob1977.txt             \n",
      "  inflating: yob1978.txt             \n",
      "  inflating: yob1979.txt             \n",
      "  inflating: yob1980.txt             \n",
      "  inflating: yob1981.txt             \n",
      "  inflating: yob1982.txt             \n",
      "  inflating: yob1983.txt             \n",
      "  inflating: yob1984.txt             \n",
      "  inflating: yob1985.txt             \n",
      "  inflating: yob1986.txt             \n",
      "  inflating: yob1987.txt             \n",
      "  inflating: yob1988.txt             \n",
      "  inflating: yob1989.txt             \n",
      "  inflating: yob1990.txt             \n",
      "  inflating: yob1991.txt             \n",
      "  inflating: yob1992.txt             \n",
      "  inflating: yob1993.txt             \n",
      "  inflating: yob1994.txt             \n",
      "  inflating: yob1995.txt             \n",
      "  inflating: yob1996.txt             \n",
      "  inflating: yob1997.txt             \n",
      "  inflating: yob1998.txt             \n",
      "  inflating: yob1999.txt             \n",
      "  inflating: yob2000.txt             \n",
      "  inflating: yob2001.txt             \n",
      "  inflating: yob2002.txt             \n",
      "  inflating: yob2003.txt             \n",
      "  inflating: yob2004.txt             \n",
      "  inflating: yob2005.txt             \n",
      "  inflating: yob2006.txt             \n",
      "  inflating: yob2007.txt             \n",
      "  inflating: yob2008.txt             \n",
      "  inflating: yob2009.txt             \n",
      "  inflating: yob2010.txt             \n",
      "  inflating: yob2011.txt             \n",
      "  inflating: yob2012.txt             \n",
      "  inflating: yob2013.txt             \n",
      "  inflating: yob2014.txt             \n",
      "  inflating: yob2015.txt             \n",
      "  inflating: yob2016.txt             \n",
      "  inflating: yob1880.txt             \n",
      "  inflating: yob1881.txt             \n",
      "  inflating: yob1882.txt             \n",
      "  inflating: yob1883.txt             \n",
      "  inflating: NationalReadMe.pdf      \n"
     ]
    }
   ],
   "source": [
    "! mkdir data ;  cd data ; wget https://www.ssa.gov/oact/babynames/names.zip ; unzip names.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mv data/yob2016.txt data/test_data.txt\n",
    "! cat data/yob* > data/allnames.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "\n",
    "filename = 'data/allnames.txt'\n",
    "df=pd.read_csv(filename, sep=',', names = [\"Name\", \"Gender\", \"Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the data size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1859026, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 189K rows and 3 columns. Now lets see how the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna</td>\n",
       "      <td>F</td>\n",
       "      <td>2604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emma</td>\n",
       "      <td>F</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>F</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Minnie</td>\n",
       "      <td>F</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Margaret</td>\n",
       "      <td>F</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ida</td>\n",
       "      <td>F</td>\n",
       "      <td>1472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alice</td>\n",
       "      <td>F</td>\n",
       "      <td>1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bertha</td>\n",
       "      <td>F</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>F</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name Gender  Count\n",
       "0       Mary      F   7065\n",
       "1       Anna      F   2604\n",
       "2       Emma      F   2003\n",
       "3  Elizabeth      F   1939\n",
       "4     Minnie      F   1746\n",
       "5   Margaret      F   1578\n",
       "6        Ida      F   1472\n",
       "7      Alice      F   1414\n",
       "8     Bertha      F   1320\n",
       "9      Sarah      F   1288"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set has 3 columns, Name, Gender, and count. Here Count is the number of times this name was registered with the \n",
    "United States social security department. The names sound familiar for United states. Since we collected data\n",
    "from all 50 states, there might be some names that occur multiple times. Lets us check how many time Mary occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>6919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3935</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>8148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>8012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7407</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8146</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610</th>\n",
       "      <td>Mary</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name Gender  Count\n",
       "0     Mary      F   7065\n",
       "1273  Mary      M     27\n",
       "2000  Mary      F   6919\n",
       "3238  Mary      M     29\n",
       "3935  Mary      F   8148\n",
       "5276  Mary      M     30\n",
       "6062  Mary      F   8012\n",
       "7407  Mary      M     32\n",
       "8146  Mary      F   9217\n",
       "9610  Mary      M     36"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Name'] == 'Mary'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at sample data\n",
    "The name 'Mary' occurs multple times, and at the same time Mary is also \n",
    "listed as a Male. In the early 20th century Mary used to be a\n",
    "common name for boys, and it somewhat related to Mario.\n",
    "But, looking at the counts, Mary is much more popular \n",
    "as a female name than a male name. So, it is not possible to \n",
    "guess the gender of a person by just looking at it. \n",
    "\n",
    "The second problem is that, the name Mary appears multple times \n",
    "in the dataset. We will remove redundant entries. \n",
    "But before we remove redundant entries, we will drop the counts as \n",
    "we will not be using it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since we do not need the 'count' lets drop it from the dataframe\n",
    "df = df.drop(['Count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#checking the presence of Mary again\n",
    "df.loc[df['Name'] == 'Mary']\n",
    "\n",
    "#lets shuffle the data set\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names in the training dataset 105431\n"
     ]
    }
   ],
   "source": [
    "# lets find the number of rows we have now. We want to \n",
    "# have a reasonable number to rows to train our deep learning model\n",
    "num_names = df.shape[0]\n",
    "print ('Number of names in the training dataset', num_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest name: 15\n"
     ]
    }
   ],
   "source": [
    "# Find the longest name\n",
    "max_name_length = (df['Name'].map(len).max())\n",
    "print(\"Longest name:\", max_name_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘namesdata’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir namesdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('namesdata/train_names.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = 'data/test_data.txt'\n",
    "df_test=pd.read_csv(test_file, sep=',', names = [\"Name\", \"Gender\", \"Count\"])\n",
    "df_test = df_test.drop(['Count'], axis=1)\n",
    "df_test.to_csv('namesdata/test_names.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32868, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption\n",
    "Beyond this point, this model will assume that the names only contain\n",
    "english alphabets (26). The algorithm has to be modified slightly if you \n",
    "use the same model for other languages.\n",
    "\n",
    "# One hot encoding of characters\n",
    "We cannot use the character symbols as is to send as input to the neural network,\n",
    "so we will convert this into a one-hot encoded sequence, based on the mapping.\n",
    "\n",
    "First lets encode the character as integer and then encode the integers into one-hot \n",
    "In one-hot encodeing a is represented as an array with the first column selected and so on \n",
    "\n",
    "a => [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "\n",
    "e => [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets define a dictionar to help us with char to integer encoding\n",
    "char_to_int = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X will be the input to the neural network, is a 3D numpyarray.\n",
    "# X is initialized with zeros\n",
    "alphabet_size = 26\n",
    "names = df['Name'].values\n",
    "genders = df['Gender']\n",
    "X = np.zeros((num_names, max_name_length, alphabet_size))\n",
    "\n",
    "# we will in each column we will encode 1 in in the column that represents the character\n",
    "for i,name in enumerate(names):\n",
    "    name = name.lower()\n",
    "    for t, char in enumerate(name):\n",
    "        X[i, t,char_to_int[char]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first name is:  Devette\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the first name\n",
    "# every name will be of the same size 26 x 15. IN case of the \n",
    "# first name 'Mary' only the first 4 letters will be encoded\n",
    "# the rest of the rows will be all zeros\n",
    "\n",
    "print ('first name is: ', names[0])\n",
    "X[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets encode the gender in a numpy array Y\n",
    "Y = np.ones((num_names,1))\n",
    "Y[df['Gender'] == 'F',0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training job setup\n",
    "The above exercise was only to show you how to create the input and target \n",
    "for the model. We will not be training in this notebook instance, but will \n",
    "submit a training job to sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-514539217087\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='namesdata', key_prefix='namesdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# todo draw a picture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-514539217087\n",
      "INFO:sagemaker:Creating training-job with name: tf-names-2018-03-20-23-43-30-553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:TensorBoard 0.1.7 at http://localhost:6006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:11,122 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:11,122 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:13,808 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:14,971 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:15,093 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:15,141 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:15,142 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:15,142 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-1-514539217087/tf-names-2018-03-20-23-43-30-553/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0cd3da290>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_num_worker_replicas': 1, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:15,143 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:15,143 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mMonitors are deprecated. Please use tf.train.SessionRunHook.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /opt/ml/code/gender-keras-2.py:47: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPlease switch to tf.train.get_global_step\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:18.603921: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:18.740547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:18.740889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \u001b[0m\n",
      "\u001b[31mname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8755\u001b[0m\n",
      "\u001b[31mpciBusID: 0000:00:1e.0\u001b[0m\n",
      "\u001b[31mtotalMemory: 11.17GiB freeMemory: 11.10GiB\u001b[0m\n",
      "\u001b[31m2018-03-20 23:51:18.740917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-us-east-1-514539217087/tf-names-2018-03-20-23-43-30-553/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.69289005, step = 1\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.8531\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6525104, step = 101 (6.733 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4505\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.66852105, step = 201 (6.472 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4686\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.63136244, step = 301 (6.465 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4407\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.62283534, step = 401 (6.477 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4711\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.62456816, step = 501 (6.464 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.433\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6616365, step = 601 (6.480 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.443\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6206199, step = 701 (6.476 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4214\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6606257, step = 801 (6.484 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4434\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.65206003, step = 901 (6.475 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Starting evaluation at 2018-03-20-23:52:33\u001b[0m\n",
      "\u001b[31m2018-03-20 23:52:33.393951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-1-514539217087/tf-names-2018-03-20-23-43-30-553/checkpoints/model.ckpt-1\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [1/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [2/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [3/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [4/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [5/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [6/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [7/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [8/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [9/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [10/10]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Finished evaluation at 2018-03-20-23:52:36\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving dict for global step 1: accuracy = 0.0, global_step = 1, loss = 0.69052064\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Validation (step 1000): loss = 0.69052064, global_step = 1, accuracy = 0.0\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 7.58179\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.64436585, step = 1001 (13.189 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4455\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.7071104, step = 1101 (6.475 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4402\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6105825, step = 1201 (6.477 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4383\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.64340734, step = 1301 (6.477 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4193\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.63523436, step = 1401 (6.485 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4371\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.63696826, step = 1501 (6.478 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4225\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.65144044, step = 1601 (6.484 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4218\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.65839326, step = 1701 (6.886 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.5253\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6106005, step = 1801 (6.483 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4217\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6996114, step = 1901 (6.485 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.4545\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6345346, step = 2001 (6.918 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4164\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6730752, step = 2101 (6.487 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.435\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.60609543, step = 2201 (6.479 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4366\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.72050536, step = 2301 (6.478 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4141\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6045669, step = 2401 (6.488 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4063\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6153757, step = 2501 (6.491 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4255\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6211268, step = 2601 (6.483 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4336\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6414144, step = 2701 (6.479 sec)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4196\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6135973, step = 2801 (6.485 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4203\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6837684, step = 2901 (6.485 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.6138\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.64373237, step = 3001 (6.843 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.2277\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6260154, step = 3101 (6.567 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.3528\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6590432, step = 3201 (6.513 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4166\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.6192204, step = 3301 (6.486 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4333\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.5579969, step = 3401 (6.479 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4092\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.62476283, step = 3501 (6.490 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4177\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.57505774, step = 3601 (6.859 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 14.5685\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.5770806, step = 3701 (6.491 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 15.4222\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.63677657, step = 3801 (6.484 sec)\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-4d88cf945269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                base_job_name='tf-names')\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgender_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_tensorboard_locally\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, run_tensorboard_locally)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mfit_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36mfit_super\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m         \"\"\"\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfit_super\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_tensorboard_locally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJOB_NAME_PARAM_NAME\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSAGEMAKER_REGION_PARAM_NAME\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFramework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Asynchronous fit not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOB_COMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "gender_estimator = TensorFlow(entry_point='gender-keras-2.py',\n",
    "                               role=role,\n",
    "                               training_steps= 20000,                                  \n",
    "                               evaluation_steps= 10,\n",
    "                               hyperparameters={'learning_rate': 0.01},\n",
    "                               train_instance_count=1,\n",
    "                               train_instance_type='ml.p2.xlarge',\n",
    "                               base_job_name='tf-names')\n",
    "\n",
    "gender_estimator.fit(inputs, run_tensorboard_locally=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To load a preexisting model\n",
    "\n",
    "#from sagemaker.tensorflow import TensorFlowPredictor\n",
    "#predictor = TensorFlowPredictor('sagemaker-tensorflow-py2-cpu-2018-03-09-19-27-43-438')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: tensorboard-names-2018-03-20-22-40-47-154\n",
      "INFO:sagemaker:Creating endpoint with name tensorboard-names-2018-03-20-22-40-47-154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "gender_predictor = gender_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'swetha'\n",
    "data = np.zeros((1,15,26), dtype=np.float32)\n",
    "char_indices = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25}\n",
    "\n",
    "for t, char in enumerate(name):\n",
    "    data[0,t,char_to_int[char]] = 1\n",
    "tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[1,15,26], dtype=tf.float32)\n",
    "output = gender_predictor.predict(tensor_proto)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = {}\n",
    "data['name'] = 'david'\n",
    "json_data = json.loads('{\"name\":\"david\"}')\n",
    "json_obj = json.loads('{\"name\":\"david\"}')\n",
    "#tensor_proto = tf.make_tensor_proto(values=json_obj)\n",
    "output = gender_predictor.predict(json_data)\n",
    "#print(json_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: tensorboard-names-2018-03-20-19-18-18-841\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(gender_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name1': 'pratap', 'name2': 'swetha'}\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "data['name'] = 'pratap'\n",
    "json_obj = json.loads('{\"names\": {\"name1\":\"pratap\",\"name2\":\"swetha\"}}')\n",
    "json_data = json.dumps(data)\n",
    "print (json_obj['names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ContentType\": \"*/*\",\n",
      "    \"InvokedProductionVariant\": \"AllTraffic\"\n",
      "}\n",
      "{\n",
      "  \"outputs\": {\n",
      "    \"Gender\": {\n",
      "      \"dtype\": \"DT_FLOAT\", \n",
      "      \"floatVal\": [\n",
      "        0.17971082031726837\n",
      "      ], \n",
      "      \"tensorShape\": {\n",
      "        \"dim\": [\n",
      "          {\n",
      "            \"size\": \"1\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!rm output.json\n",
    "!aws sagemaker-runtime invoke-endpoint --endpoint-name tensorboard-names-2018-03-20-22-40-47-154 --body '{\"name\":\"barnam\"}' --content-type \"application/json\" output.json\n",
    "! cat output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowPredictor\n",
    "predictor = TensorFlowPredictor('tensorflowgendermodel571')\n",
    "sagemaker.Session().delete_endpoint(predictor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
